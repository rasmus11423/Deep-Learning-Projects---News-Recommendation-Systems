{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization & Model Prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from models.train import grab_data,grab_embeded_articles,initialize_model,train_model,validate_model,grab_data_test,predict_model\n",
    "from models.dataloader_pytorch import NRMSDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL\n",
    ")\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from utils._behaviors import (\n",
    "    add_prediction_scores\n",
    ")\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "\n",
    "# Create the parser\n",
    "args = Namespace(debug=True, dataset=\"my_dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters \n",
    "HISTORY_SIZE = 20\n",
    "BATCH_SIZE = 64\n",
    "title_size, history_size = 30, 30\n",
    "head_num, head_dim, attention_hidden_dim, dropout = 20, 20, 200, 0.2\n",
    "lr =0.01\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.15\n",
    "dataset_name = \"ebnerd_demo\"\n",
    "df_train, df_validation = grab_data(dataset_name,HISTORY_SIZE,frac)\n",
    "df_train = df_train.head(4*BATCH_SIZE)\n",
    "df_validation = df_validation.head(4*BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path(\"/Users/astridh/Documents/GitHub/Deep-Learning-Projects---News-Recommendation-Systems?fbclid=IwZXh0bgNhZW0CMTEAAR2W1fXivVjSypOEuVjFfg6jZ5IdOeH2OkDWZcbidgezWxAkAp1PnOoHKBA_aem_LPGF8NWJj3P5GF0SIL4g2w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading articles and generating embeddings...\n"
     ]
    }
   ],
   "source": [
    "LOCAL_TOKENIZER_PATH = BASE_PATH.joinpath(\"data/local-tokenizer\")\n",
    "LOCAL_MODEL_PATH = BASE_PATH.joinpath(\"data/local-tokenizer-model\")\n",
    "\n",
    "article_mapping, word2vec_embedding = grab_embeded_articles(LOCAL_TOKENIZER_PATH,LOCAL_MODEL_PATH,dataset_name, title_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    unknown_representation=\"zeros\",\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE, \n",
    ")\n",
    "\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    unknown_representation=\"zeros\",\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in PyTorch DataLoader\n",
    "train_loader = DataLoader(train_dataloader, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataloader, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NewsEncoder and UserEncoder with all submodules...\n",
      "Model initialized with the following architecture:\n",
      "NRMSModel(\n",
      "  (user_encoder): UserEncoder(\n",
      "    (titleencoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 768)\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (self_attention): SelfAttention()\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (att_layer): AttLayer2()\n",
      "    )\n",
      "    (self_attention): SelfAttention()\n",
      "    (attention): AttLayer2()\n",
      "  )\n",
      "  (news_encoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (self_attention): SelfAttention()\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    (att_layer): AttLayer2()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = initialize_model(word2vec_embedding, title_size, HISTORY_SIZE, head_num, head_dim, attention_hidden_dim, dropout)\n",
    "model.to(device)\n",
    "# Set up optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # with added weight decay\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4/4 [00:18<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History size in dataloader: torch.Size([64, 20, 30]), NPRatio: torch.Size([64, 5, 30])\n",
      "Epoch 1: Train Loss = 0.4194, Train Acc = 0.8945, Train Auc = 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1: 100%|██████████| 4/4 [00:44<00:00, 11.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 442.2948, Val Acc = 0.8449, Val Auc = 0.5346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4/4 [00:19<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History size in dataloader: torch.Size([64, 20, 30]), NPRatio: torch.Size([64, 5, 30])\n",
      "Epoch 2: Train Loss = 0.3118, Train Acc = 0.9141, Train Auc = 0.6299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2: 100%|██████████| 4/4 [00:47<00:00, 11.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Val Loss = 454.1446, Val Acc = 0.8318, Val Auc = 0.5316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4/4 [00:21<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History size in dataloader: torch.Size([64, 20, 30]), NPRatio: torch.Size([64, 5, 30])\n",
      "Epoch 3: Train Loss = 0.2350, Train Acc = 0.9375, Train Auc = 0.6254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 3: 100%|██████████| 4/4 [00:45<00:00, 11.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Val Loss = 466.3376, Val Acc = 0.8272, Val Auc = 0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 4/4 [00:17<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History size in dataloader: torch.Size([64, 20, 30]), NPRatio: torch.Size([64, 5, 30])\n",
      "Epoch 4: Train Loss = 0.1930, Train Acc = 0.9492, Train Auc = 0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 4: 100%|██████████| 4/4 [00:38<00:00,  9.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Val Loss = 477.2268, Val Acc = 0.8211, Val Auc = 0.5298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 4/4 [00:17<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History size in dataloader: torch.Size([64, 20, 30]), NPRatio: torch.Size([64, 5, 30])\n",
      "Epoch 5: Train Loss = 0.1633, Train Acc = 0.9570, Train Auc = 0.6238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 5: 100%|██████████| 4/4 [00:41<00:00, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Val Loss = 486.2350, Val Acc = 0.8148, Val Auc = 0.5298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    with tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\") as pbar:\n",
    "            train_loss, train_acc, train_auc = train_model(pbar, model, criterion, optimizer, device, args, \"run\")\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Train Auc = {train_auc:.4f}\")\n",
    "\n",
    "    # Validate the model\n",
    "    with tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}\") as pbar:\n",
    "            val_loss, val_acc, val_auc = validate_model(pbar, model, criterion, device,args, \"run\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}, Val Auc = {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting batches: 100%|██████████| 4/4 [00:42<00:00, 10.74s/it]\n",
      "/var/folders/ts/rwxl4zb93kn8x887pp3khy4h0000gn/T/ipykernel_9542/4227404329.py:26: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_test = df_test.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking predictions for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 41674.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping data/predictions/test_predictions.txt to data/predictions/test_predictions.zip\n",
      "Test predictions saved at: data/predictions/test_predictions.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from the model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "pred_validation = []\n",
    "\n",
    "# Load the test dataset\n",
    "print(\"Loading test dataset...\")\n",
    "df_test = grab_data_test(dataset_name=\"ebnerd_testset\", history_size=HISTORY_SIZE) #.head(4*BATCH_SIZE)\n",
    "df_test = df_test.head(4*BATCH_SIZE)\n",
    "\n",
    "# Create Test Dataloader\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    unknown_representation=\"zeros\",\n",
    "    eval_mode=True,  # Test mode\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "pred_test = predict_model(test_dataloader, model, device)\n",
    "# Add prediction scores\n",
    "df_test = add_prediction_scores(df_test, pred_test.tolist())\n",
    "\n",
    "# Rank predictions\n",
    "print(\"Ranking predictions for test set...\")\n",
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "\n",
    "# Write submission file\n",
    "test_output_file = Path(\"data/predictions/test_predictions.txt\")\n",
    "test_output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=str(test_output_file),\n",
    ")\n",
    "\n",
    "print(f\"Test predictions saved at: {test_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
